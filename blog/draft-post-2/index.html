<html>
<head>
<link rel="stylesheet" href="/blog/tufte.css" />
<script
src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'
async></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  // skipTags: ["<pre>", "<code>"]
  tex2jax: {
      skipTags: ["script","noscript","style","textarea"]
      }
  });
</script>
<title>TITLE</title>
</head>
<body>
    <p><a href=/blog/>To index</a></p>

    <article>
<h1>TITLE</h1><section>
<ul>
<li>learning from experience</li>
<li>driver first, using monte carlo methods</li>
<li>tic tac toe</li>
<li>driver</li>
<li>when there are too many states: give some examples, including mastermind</li>
</ul>
<p>-- nice intro--</p>
<p>Let's get up to speed with an example: racetrack driving<label class="sidenote-number"></label><span class="sidenote">taken from Sutton and
Barto, <em>reinforcement learning</em>, Chapter X, etc</span>. We'll take the famous Formula 1
racing driver Pimi Roverlainen and transplant him onto a racetrack in gridworld.</p>
<p>INSERT IMAGE</p>
<p>Pimi's task is to learn how to drive from any given point on the red starting line to any
point of his choice on the green ending line. He should do this as quickly as
possible. We'll model Pimi's driving in a simplified way: namely as a sequence
of moves from square to square. After each move he gets to accelerate by +1, 0 or -1 velocity units
in either or both of the X and Y directions, giving a total of 9
possibilities<label class="sidenote-number"></label><span class="sidenote">(+1,+1), (+1, 0), (+1, -1), etc.</span>. His new velocity then
determines in which square he'll end up next. For safety, let's cap both the
X-speed and Y-speed at 5 units.</p>
<p>We can write down our task using more formal terminology. Our task is to learn the best possible <em>action</em> in the given
<em>state</em> we're in. In this case, a state is one of this possible cells in the
grid and at each cell there's 1 of 9 possible actions we can choose from. In
other words, we have to learn an ideal <em>policy</em>, an ideal mapping from states
to actions. Again:</p>
<ul>
<li><em>states</em> <code>\(S\)</code> are defined to be the possible cells in our grid</li>
<li><em>actions</em> <code>\(A\)</code> are defined to be the 9 possible accelerations we can perform at
each state</li>
<li>The <em>policy</em> is our current strategy for selecting the action given the state
we're in. This is a mapping <code>\( \pi: S \to A \)</code>. Pimi's goal is to learn the
best possible policy.</li>
</ul>
<p>On such an easy course, this is an easy task for Pimi. So, to make things harder,
we'll blindfold him so he cannot see where he's going. All he has access to is
9 <em>success logbooks</em> at each square in the racetrack -- so <code>\(9N\)</code> logbooks,
where <code>\(N\)</code> is the number of squares in the track. Each logbook belongs to 1 of
the 9 possible accelerations Pimi could make, say for example <code>\((+1, 0)\)</code>. Then
this logbook <code>\((+1, 0)\)</code> has a
full record of the number of moves it's taken Pimi to
arrive at the green line in the past starting at the current square, given
that the action taken was <code>\((+1, 0)\)</code>.</p>
<p>As Pimi drives with his blindfold on, he drives at random by accelerating in
random directions because he can't see where he's going. He often crashes and
when this happens, he takes a sip of refreshing blueberry juice, fetches a
plaster or two and then starts again from a random point on the red starting line,
keeping a stiff upper lip. As he does so, he crosses more and more squares, the
smoke rises, the racetrack gets worn and the <code>\(9N\)</code> logbooks fill up.
That's a lot of paper!</p>
<p>Once there's a fair amount of records in each of the <code>\(9N\)</code> logbooks, Pimi can
start using them to make decisions instead of driving and crashing randomly.
Each success logbook is indeed a measure of success of its corresponding action at that
square in gridworld: the logbook averaging the <em>lowest</em> number of moves
to completion should intuitively correspond to the best action. Why? Because
the logbooks tell us exactly which of the 9 outcomes has, on average, a more
desirable outcome. If for example logbook 3 corresponding to action <code>\((+1, -1)\)</code> averages
87 moves to completion, and logbook 2 corresponding to action <code>\((+1, 0)\)</code>
averages 56 moves to completion, then choosing <code>\((+1, -1)\)</code> is measurably
better than choosing <code>\((+1, 0)\)</code>.</p>
<p>This intution is in fact correct and has solid theoretical
underpinnings<label class="sidenote-number"></label><span class="sidenote">Barto and Sutton, Chapters 3-5</span>.
The particular approach just outlined is an example of a class of
algorithms called <em>Monte Carlo algorithms</em> in reinforcement learning. These
algorithms learn the <em>value</em> of a particular action <code>\(a\)</code> taken at a state
<code>\(s\)</code> by running many trials and evaluating the consequence for each trial.
In our case this means letting Pimi race again and again and counting the
number of moves it takes him to complete the track each time. The final <em>value</em>
of action <code>\(a\)</code> at state <code>\(s\)</code> , denoted <code>\(q(s, a)\)</code>, is then the
average length of time to completion after that action, averaged over all of
the trials. We can then use these value estimates to <em>update</em> our policy, our
strategy for selecting actions. Pimi takes his logbooks, calculates the best
move at each step and then picks <em>that</em> move instead of making a random choice.
The learning problem thus breaks into 2 parts:</p>
<ul>
<li><em>value estimation</em>, which is about assigning a numeric score or measure to an
action at a given state given our current strategy. We want to learn the
value function <code>\(q_{\pi}: A \times S \to \mathbb{R}\)</code>, a function from
state-action pairs to real numbers. The subscript <code>\(\pi\)</code> indicates the
fact that this function depends on our current policy.</li>
<li><em>policy iteration</em>, which is about updating our policy based on our value
estimates. We update our policy to choose at each stage the best action given
our current value estimate.</li>
</ul>
<p>Finding an ideal policy is an iterative process, which involves repeating these
steps over and over again. We start with a random policy and calculate the
values of actions given this behaviour. This tells us which actions are
optimal -- under our current, <em>random</em> behaviour -- and so we modify our policy
to choose instead these better values. A subtle but crucial point here is that
our value estimates are always with respect to our <em>current policy</em>, not the
ideal policy. When we perform policy iteration, we move further towards the
ideal policy because we weed out bad decisions: we avoid choosing actions which
our value estimates tell us are measurably bad. But, crucially, our value
estimates don't automatically give us the ideal policy. They merely point us in
the right direction -- <em>up to a point</em>. That's why we should always distrust a
bit of our value estimates when updating our policy. Value estimates tell us
how we can improve the policy for short term gain. However, they don't tell us
directly what is the ideal policy. Always going for the short term gain can
preclude long-term, big benefits. For this reason, it's good always to
<em>explore</em> when performing policy iteration and make moves different to the ones
pointed to by our value estimates. In practice this means updating our policy
so that it follows the advice given by our value estimates <em>most</em> of the time --
this is known as <em>exploitation</em> -- but still selecting  a random move
sometimes. This last part is known as <em>exploration</em>. The exact proportion
<code>\(\epsilon\)</code> of moves that are exploratory is a parameter in our algorithm.
Different values of <code>\(\epsilon\)</code> may lead to different results: a better
policy or faster / slower convergence to the optimum policy. As a rule of
thumb, a higher value of <code>\(\epsilon\)</code> can lead to a better policy, but it'll
take longer to find it: slower convergence. This is a tradeoff between
exploration and exploitation and has the fancy name <em>exploration-exploitation
tradeoff.</em></p>
<p>I've implemented a Monte Carlo algorithm for the racetrack and here are my
results:</p>
<p>INSERT IMAGE</p>
<p>The implementation is on <a href="/blog/draft-post-2/bla">GitHub</a> and I encourage you to take a look.
The main components are here:</p>
</section></article>

    <p><a href=/blog/>To index</a></p>

    <div class="dates">
        First published 28 September 2018 at 14:53 UTC, <br />
        last updated 30 September 2018 at 14:41 UTC.
    </div>
</body>
</html>
