<html>
<head>
    <meta name="viewport" content="width=device-width,
    initial-scale=1.0"></meta>
<link rel="stylesheet" href="/blog/tufte.css" />
<link rel="stylesheet" href="/blog/intro-reinforcement-learning-tic-tac-toe/article.css" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-134129591-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-134129591-1');
</script>

<script
src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'
async></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  // skipTags: ["<pre>", "<code>"]
  tex2jax: {
      skipTags: ["script","noscript","style","textarea"]
      },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
<title>A Brisk Intro to Reinforcement Learning</title>
</head>
<body>
    <p>
        <a href="/blog/">&lt;Index</a>
        <a href="/blog/rss.xml">RSS</a>
        <a href="/">Home</a>
    </p>

    <article>
<h1>A Brisk Intro to Reinforcement Learning</h1><section>
<p>How do humans learn? Unlike a successful <a href="http://image-net.org/">image classification
pipeline</a>, humans do not require millions of labelled
examples to distinguish chairs from tables. Instead, we learn by interacting
with the environment, requiring no labelled supervision at all. How is this
possible, and can it be replicated on a machine?</p>
<p>This spring I had the privilege of attending <a href="https://www.recurse.com/scout/click?t=ae23300ce9c535472ec760f1f5c34bab">The Recurse
Center</a> in New York City and one of the topics I was keenest to explore is <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>. Reinforcement learning algorithms teach <em>agents</em> by letting them interact with an <em>environment</em>, much like humans do. What do these algorithms look like?</p>
<h2>Monte Carlo RL: The Racetrack</h2>
<p>Let's get up to speed with an example: racetrack driving. We'll take the famous Formula 1
racing driver Pimi Roverlainen and transplant him onto a racetrack in gridworld.</p>
<p><img src="/blog/intro-reinforcement-learning-tic-tac-toe/racetrack-full-example.png" alt="" /><br /><span class="image-caption">The idealised racetrack</span></p>
<p>Pimi's task is to learn how to drive from any given point on the red starting line to any
point of his choice on the green finish line. He should do this as quickly as
possible. We'll model Pimi's driving in a simplified way: namely as a sequence
of moves from square to square. After each move he gets to accelerate by +1, 0 or -1 velocity units
in either or both of the X and Y directions, giving a total of 9
possibilities<label class="sidenote-number"></label><span class="sidenote">(+1,+1), (+1, 0), (+1, -1), etc.</span>. His new velocity then
determines in which square he'll end up next. For safety, let's cap both the
X-speed and Y-speed at 5 units.</p>
<p><em>This example is taken from the book &quot;Reinforcement Learning&quot; by Richard Sutton and
Andrew Barto, 2018 Edition, Chapter 5.7, p111. I'll be referring to this book
frequently and absolutely recommend it as an introduction to reinforcement
learning.</em></p>
<p>We can write down our task using more formal terminology. Our task is to learn the best possible <em>action</em> in the given
<em>state</em> we're in. In this case, a state is one of this possible cells in the
grid and at each cell there's 1 of 9 possible actions we can choose from. In
other words, we have to learn an ideal <em>policy</em>, an ideal mapping from states
to actions. Again:</p>
<ul>
<li><em>states</em> <code>\(S\)</code> are defined to be the possible cells in our grid</li>
<li><em>actions</em> <code>\(A\)</code> are defined to be the 9 possible accelerations we can perform at
each state</li>
<li>The <em>policy</em> is our current strategy for selecting the action given the state
we're in. This is a mapping <code>\( \pi: S \to A \)</code>. Pimi's goal is to learn the
best possible policy.</li>
</ul>
<p>On such an easy course, this is an easy task for Pimi. So, to make things harder,
we'll blindfold him so he cannot see where he's going. All he has access to is
9 <em>success logbooks</em> at each square in the racetrack -- so <code>\(9N\)</code> logbooks,
where <code>\(N\)</code> is the number of squares in the track. Each logbook belongs to 1 of
the 9 possible accelerations Pimi could make, say for example <code>\((+1, 0)\)</code>. Then
this logbook <code>\((+1, 0)\)</code> has a
full record of the number of moves it's taken Pimi to
arrive at the green line in the past starting at the current square, given
that the action taken was <code>\((+1, 0)\)</code>.</p>
<p>As Pimi drives with his blindfold on, he drives at random by accelerating in
random directions because he can't see where he's going. He often crashes and
when this happens, he takes a sip of refreshing blueberry juice, fetches a
plaster or two and then starts again from a random point on the red starting line,
keeping a stiff upper lip. As he does so, he crosses more and more squares, the
smoke rises, the racetrack gets worn and the <code>\(9N\)</code> logbooks fill up.
That's a lot of paper!</p>
<p>Once there's a fair amount of records in each of the <code>\(9N\)</code> logbooks, Pimi can
start using them to make decisions instead of driving and crashing randomly.
Each success logbook is indeed a measure of success of its corresponding action at that
square in gridworld: the logbook averaging the <em>lowest</em> number of moves
to completion should intuitively correspond to the best action. Why? Because
the logbooks tell us exactly which of the 9 actions has, on average, a more
desirable outcome. If for example logbook 3 corresponding to action <code>\((+1, -1)\)</code> averages
87 moves to completion, and logbook 2 corresponding to action <code>\((+1, 0)\)</code>
averages 56 moves to completion, then choosing <code>\((+1, 0)\)</code> is measurably
better than choosing <code>\((+1, -1)\)</code>.</p>
<p>This intution is in fact correct and has solid theoretical
underpinnings<label class="sidenote-number"></label><span class="sidenote">Sutton and Barto, Chapters 3-5</span>.
The particular approach just outlined is an example of a class of
algorithms called <em>Monte Carlo algorithms</em> in reinforcement learning. These
algorithms learn the <em>value</em> of a particular action <code>\(a\)</code> taken at a state
<code>\(s\)</code> by running many trials and evaluating the consequence for each trial.
In our case this means letting Pimi race again and again and counting the
number of moves it takes him to complete the track each time. The final <em>value</em>
of action <code>\(a\)</code> at state <code>\(s\)</code> , denoted <code>\(q(s, a)\)</code>, is then the
average length of time to completion after that action, averaged over all of
the trials. We can then use these value estimates to <em>update</em> our policy, our
strategy for selecting actions. Pimi takes his logbooks, calculates the best
move at each step and then picks <em>that</em> move instead of making a random choice.
The learning problem thus breaks into 2 parts:</p>
<ul>
<li><em>value estimation</em>, which is about assigning a numeric score or measure to an
action at a given state given our current strategy. We want to learn the
value function <code>\(q_{\pi}: A \times S \to \mathbb{R}\)</code>, a function from
state-action pairs to real numbers. The subscript <code>\(\pi\)</code> indicates the
fact that this function depends on our current policy. This is also known as
the <em>prediction</em> problem, because we are predicting the values.</li>
<li><em>policy iteration</em>, which is about updating our policy based on our value
estimates. We update our policy to choose at each stage the best action given
our current value estimate. This is also known as the <em>control</em> problem.</li>
</ul>
<p>Finding an ideal policy is an iterative process, which involves repeating these
steps over and over again. We start with a random policy and calculate the
values of actions given this behaviour. This tells us which actions are
optimal -- under our current, <em>random</em> behaviour -- and so we modify our policy
to choose instead these better values. A subtle but crucial point here is that
our value estimates are always with respect to our <em>current policy</em>, not the
ideal policy. When we perform policy iteration, we move further towards the
ideal policy because we weed out bad decisions: we avoid choosing actions which
our value estimates tell us are measurably bad. But, crucially, our value
estimates don't automatically give us the ideal policy. They merely point us in
the right direction -- <em>up to a point</em>. That's why we should always distrust a
bit of our value estimates when updating our policy. Value estimates tell us
how we can improve the policy for short term gain. However, they don't tell us
directly what is the ideal policy. Always going for the short term gain can
preclude long-term, big benefits. For this reason, it's good always to
<em>explore</em> when performing policy iteration. <em>Exploring</em> means making a move different to the one
pointed to by our value estimates. We update our policy
so that it follows the advice given by our value estimates <em>most</em> of the time --
this is known as <em>exploitation</em> -- but still select a random move
sometimes. This last part is known as <em>exploration</em>. The exact proportion
<code>\(\epsilon\)</code> of moves that are exploratory is a parameter in our algorithm.
Different values of <code>\(\epsilon\)</code> may lead to different results: a better
policy or faster / slower convergence to the optimum policy. As a rule of
thumb, a higher value of <code>\(\epsilon\)</code> can lead to a better policy, but it'll
take longer to find it: slower convergence. This is a tradeoff between
exploration and exploitation and has the fancy name <em>exploration-exploitation
tradeoff.</em></p>
<p>These are all the components we need to implement our reinforcement learning
algorithm. Here they are again:</p>
<ul>
<li>An <em>initial policy</em> <code>\(\pi\)</code> to start racing. In our case, this is the random policy.</li>
<li>A method to produce <em>value estimates</em> <code>\(q_\pi\)</code> for any given policy. These assign a
score to each action at each state.
In our case, this is the Monte Carlo method.</li>
<li>A way to perform <em>policy iteration</em>, in other words to update an existing
policy <code>\(\pi\)</code> and produce an improved policy <code>\(\pi'\)</code> using the value
estimates <code>\(q_\pi\)</code>. What we do is a variant of <em>general policy iteration</em>,
and we update the policy after each <em>episode</em>. An <em>episode</em> is one run of the
game: one run of Pimi on the racetrack. And we improve by defining our policy
to use the action with the <em>highest</em> value estimate at each state 90% of the
time. The remaining 10% of the time we try out random actions to retain a
healthy amount of exploration.</li>
</ul>
<p>So far the policy was a function from states to actions. We can also write it
as a funcion
<code>\(\pi(a|s) := \pi(a, s)\)</code> from state-action pairs to the real numbers. In
this case we interpret the policy
to be a <em>probability distribution</em> over <code>\(a\)</code> from which we can sample. It's
a probability distribution and so <code>\(\sum_a \pi(a|s) = 1 \)</code> for all <code>\(s\)</code>.</p>
<p>That's a lot of words. Here's some pseudocode to make things clearer<label class="sidenote-number"></label><span class="sidenote">adapted
from Sutton and Barto, Section 5.4, page 101</span>.</p>
<ol>
<li>Choose a small exploration parameter <code>\(\epsilon\)</code>, for example 0.1</li>
<li>Set the initial policy <code>\(\pi\)</code> to be the random policy</li>
<li>Initialise our value estimates arbitrarily: set <code>\(Q(s, a) \in \mathbb{R}\)</code>
arbitrarily for all <code>\(a\)</code> in <code>\(A\)</code>, <code>\(s\)</code> in <code>\(S\)</code>.</li>
<li>Set <code>\(\text{Returns}(s, a) \leftarrow \)</code> empty list, for all <code>\(a\)</code> in <code>\(A\)</code>, <code>\(s\)</code> in <code>\(S\)</code>. These <em>returns</em> are the entries in our 'success logbooks'.</li>
<li>Repeat forever:
<ol>
<li>Generate an episode according to <code>\(\pi: S_0, A_0, S_1, A_1 \ldots S_T\)</code>. This is one run across the racetrack until we hit the green finish line.</li>
<li><code>\(G \leftarrow 0\)</code></li>
<li>Loop for each episode, <code>\(T=t-1, t-2, \ldots 0\)</code>
<ol>
<li><code>\(G \leftarrow G+1\)</code></li>
<li>If the pair <code>\((S_t, A_t)\)</code> does <strong>not</strong> appear in <code>\((S_0, A_0), (S_1, A_1), \ldots (S_{t-1}, A_{t-1})\)</code>:<label class="sidenote-number"></label><span class="sidenote">this condition gives what's
called <em>first-visit Monte Carlo</em>. This is one variant of Monte Carlo
prediction. Another is <em>multiple visits</em>, which does not have this
extra 'if' condition.</span>
<ol>
<li>
<p>Append <code>\(G\)</code> to Returns<code>\((s, a)\)</code>.</p>
</li>
<li>
<p><code>\(Q(S_t, A_t) \leftarrow \)</code> average(Returns(<code>\(S_t, A_t\)</code>))</p>
</li>
<li>
<p><code>\(A^* \leftarrow \text{arg max}_a Q(S_t, a)\)</code>. Break ties
arbitrarily.</p>
</li>
<li>
<p>For all <code>\(a \in A(S_t)\)</code>, where <code>\(A(S_t)\)</code> is the set of
possible actions we can take at <code>\(S_t\)</code>:</p>
<pre class="code"><code>\[
\pi(a|S_t) \leftarrow = \begin{cases}
    1 - \epsilon + \epsilon / |A(S_t)| &amp; \text{if } a = A^* \\
    \epsilon / |A(S_t)| &amp; \text{otherwise}
\end{cases}
\]
</code></pre>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2>RaceTrack Implementation</h2>
<p>I've implemented a Monte Carlo algorithm in
<a href="https://julialang.org/">Julia</a>
for the racetrack and here are my
results:</p>
<p><img src="/blog/intro-reinforcement-learning-tic-tac-toe/racetrack.gif" alt="" /><br /><span class="image-caption">RaceTrack Result</span></p>
<p>The implementation is on
<a href="https://github.com/egeromin/Reinforce.jl/blob/master/RaceTrack.jl">GitHub</a> and I encourage you to take a look.
Let's zoom in and take a closer look at the most important parts.</p>
<pre class="code"><code>&quot;&quot;&quot;
On-policy monte-carlo control
&quot;&quot;&quot;
function monte_carlo_control(track::Track, num_episodes::Int, eps::Float64)

    qval = QValues()
    returns = StateActionReturns()

    episode_lengths = []

    for i in 1:num_episodes
        episode = generate_episode(track, eps, qval)
        @assert isa(episode, Episode)
        qval = update_state_returns!(episode, eps, returns, track)
        push!(episode_lengths, length(episode))

        if i % 1000 == 0
            println(@sprintf &quot;Done %i of %i; episode length %i&quot; i num_episodes length(episode))
        end
    end

    return qval, episode_lengths
end
</code></pre>
<p>This is the heart of the algorithm: our control loop. Our policy is determined
by a table of Q-Values. This is our function <code>\(q_\pi\)</code> from before. We
populate our table by setting every entry to zero, so <code>\(q_\pi(s, a) = 0\)</code> for
all <code>\(s, a\)</code>. This is done by <code>qval = QValues()</code>. We also initialise our
empty lists of returns for each state action pair <code>returns = StateActionReturns()</code>.
Then we loop over a fixed number of episodes. In the pseudocode, this is an
infinite loop. At each stage, we generate a new episode given our current
qvalues: <code>episode = generate_episode(track, eps, qval)</code>. Then we update our
qvalues and our return lists: <code>qval = update_state_returns!(episode, eps, returns track)</code>. This updates our policy.</p>
<p>The control loop splits into 2 core ingredients. The first core ingredient is the <code>generate_episode</code>
function which generates an episode. The second is the <code>update_state_returns!</code> function
which take an episode and use it to update the returns lists.</p>
<pre class="code"><code>function generate_episode(track::Track, eps::Float64, q::QValues)

    episode::Episode = []
    state = random_start(track)

    for i in 1:max_episode_length
        action = epsilon_greedy(eps, q, state)
        push!(episode, (state, action))

        state = next_state(track, state, action)

        if state == TerminalState
            break
        end
    end

    push!(episode, (state, NoAction))

    return episode

end


function update_state_returns!(episode::Episode, eps::Float64,
                               r::StateActionReturns,
                               track::Track,
                               first_visit::Bool=true)

    current_return::Float64 = 0
    state, _ = episode[end]

    for i in length(episode)-1:-1:1
        if state != TerminalState &amp;&amp; track[state.position...] == red
            current_return += -50
        else
            current_return += -1
        end
        state, action = episode[i]  

        if !((state, action) in episode[1:i-1]) || (!first_visit)
            push!(r[state, action], current_return)
        end
    end

    QValues(map(mean, r.r))
end
</code></pre>
<p>These 2 functions follow the pseudocode almost exactly, with the exception that
in my implementation, I set an extra penalty for crashing (-50 instead of just
-1). The <code>generate_episode</code> function relies on 2 helper functions,
<code>epsilon_greedy</code>, which is the policy function, selecting an action given our
qvalues, and <code>next_state</code>, which evaluates the next state given what the
current state and action are and what the track shape is. I won't cover
<code>next_state</code> here in detail. It involves checking if we've hit the boundary
of the track and computing the next position given our current velocity.
<code>epsilon_greedy</code> is quite straightforward and you can check it directly in the
code.</p>
<h2>Temporal Difference RL: Tic Tac Toe</h2>
<p>Monte Carlo learning is simple and intuitive. To perform value prediction, we
run a bunch of trials and average the returns over all these trials to obtain
our estimate.</p>
<p>The method has a major drawback though: it takes up a huge amount of RAM. We
have to store all these success logbooks which fill up as Pimi races around the
track.  We have to store a return for each state-action pair
crossed in every episode. In fact, we rely on having a long list of returns for
each state-action pair because it will increase the accuracy of our estimate.
The drawback of this method is the amount of memory it requires. The greater
the accuracy, the greater the memory required.</p>
<p>An alternative class of algorithms are <em>temporal-difference (TD) algorithms</em>, which
we'll explore using a different example: Tic-Tac-Toe. Using a TD algorithm, we
can train an AI to play Tic Tac Toe. The AI learns by playing itself.</p>
<p>The basic strategy is the same:</p>
<ul>
<li>We define a value function <code>\(q(a, s)\)</code> that is used to score the desirability of any
state-action pair</li>
<li>Our policy is again an <em><code>\(\epsilon\)</code>-greedy</em> policy based on the value
function. It selects the action with the highest q-value <code>\(1-\epsilon\)</code> proportion of times
and a random action the remaining amount of times</li>
<li>We play a number of tic-tac-toe games and, as we play, we update our value
estimates based on what we've seen during each game.</li>
</ul>
<p>What changes is the last point: the strategy to update the value estimates.
Whereas with Monte Carlo methods this required storing all of the returns
following the first visit to a specific state-action pair, with TD-algorithms
we have a cheaper method. Instead of storing all of the returns for every
episode, we immediately update the q-values based only on the move that came
<em>immediately after</em> the current one. So suppose when playing a game we have a
sequence of states and actions <code>\(S_0, A_0, S_1, A_1, \ldots S_T\)</code>. Then we
update the q-values at <code>\((S_i, A_i)\)</code> thus:</p>
<pre class="code"><code>\[
Q(S_i, A_i) \leftarrow Q(S_i, A_i) + \alpha [R_{i+1} + Q(S_{i+1}, A_{i+1}) -
Q(S_i, A_i)]
\]
</code></pre>
<p>Here <code>\(\alpha\)</code> is a constant parameter, and <code>\(R_{i+1}\)</code> is called
the <em>reward</em> following action <code>\(A_i\)</code>.
Our <em>returns</em> for an episode at a given state-action pair are just the sum of rewards
following that state-action pair. In the
racetrack example, all of the rewards are set constant at -1, to incentivise
faster runs. In my modified version there's an additional penalty for crashing
because the reward is set to be -50 in that case, to disincentivise
crashing. For Tic-Tac-Toe, we set the reward to be zero while the game is not
finished and then:</p>
<ul>
<li>-1, for losing or drawing</li>
<li>1, for winning</li>
</ul>
<p>The intuition behind this update rule is that the value will 'flow' between
nodes of our state-action pair 'graph' until it reaches equilibrium. When we
have equilibrium, we have the 'true' ideal value. Of course, this
explanation is disappointingly vague. What's actually going on is that both
Monte Carlo algorithms and TD algorithms attempt to find solutions to
<em>Bellman's equation</em>, an equation that an idealised value functio obeying certain
conditions must satisfy. I won't cover this in detail, but it's at the heart of
all reinforcement learning algorithms, so be sure to check out Sutton and
Barto's book<label class="sidenote-number"></label><span class="sidenote">Section 3.5, page 58</span>.</p>
<p>In the Tic-Tac-Toe case we can cut corners further by assigning values to
<em>states</em> instead of <em>state-action pairs</em>. Whereas before we calculated a
function <code>\(Q(S, A)\)</code> for each valid <em>pair</em> <code>\((S, A)\)</code>, now we just
calculate a <em>value function</em> <code>\(V(S)\)</code> for each possible state <code>\(S\)</code>.
Our policy is still <code>\(\epsilon\)</code>-greedy with respect to <code>\(V\)</code>: it
calculates all of the next possible states we could end up in, and selects the
one with highest value <code>\(1 - \epsilon\)</code> proportion of times, and a random
valid action the remaining times. The TD update rule then becomes</p>
<pre class="code"><code>\[
V(S_i) \leftarrow V(S_i) + \alpha [R_{i+1} + V(S_{i+1}) - V(S_i)]
\]
</code></pre>
<p>Using state values instead of state-action values
further saves the amount of storage and computation we need to do, and is
perfectly valid because the desirability of a state should not change depending
on how we got there. <em>Note that this is true for the racetrack as well</em>: we could
have used state values instead of state-action values there too. The reason one
cannot always do this is that general reinforcement learning policies are
<em>stochastic</em>: the reward and next state we get back from our environment as a
result of taking a certain action A from a state S can vary, even for exactly the same A and S.</p>
<p>I've written an implementation of a TD algorithm for Tic-Tac-Toe in Julia which you can find <a href="https://github.com/egeromin/Reinforce.jl/blob/master/TicTacToe.jl">on
GitHub</a>.</p>
<p>Again we have a core training loop. Spot the updated update rule:</p>
<pre class="code"><code>function play_game!(policy_me::Policy, policy_opponent::Policy)
    @assert policy_me.player == me
    @assert policy_opponent.player == opponent
    board = Board()
    current_player = opponent
    winner = nobody
    while true
        winner = get_winner(board)
        if winner != nobody
            break
        end
        if is_board_full(board)
            break
        end

        if current_player == opponent
            board = move!(policy_opponent, board)
            current_player = me
        else
            board = move!(policy_me, board)
            current_player = opponent
        end
    end

    last_index = index_from_board(board)
    update!(policy_me, last_index)
    update!(policy_opponent, last_index)
    return winner
end


function update!(policy::LearnerPolicy, last_index::Int)
    if ! policy.update
        return
    end
    if policy.exploiting_moves[end][end] != last_index
        push!(policy.exploiting_moves[end], last_index)
    end
    for moves in policy.exploiting_moves
        for i in 2:length(moves)
            a = moves[i-1]
            b = moves[i]
            policy.values[a] = policy.values[a] + policy.alpha * (policy.values[b] - policy.values[a])
        end
    end

    policy.exploiting_moves = [[]]
end
</code></pre>
<p>I encourage you to take a look at the full code <a href="https://github.com/egeromin/Reinforce.jl/blob/master/TicTacToe.jl">on
GitHub</a>. There I compare
training against 3 possible opponents:</p>
<ul>
<li>a random opponent, which just makes random moves</li>
<li>a semi-clever opponent, whose only strategy is to fill in the last square in
the board</li>
<li>a perfect-play opponent, whose strategy is 'perfect' in that it cannot lose.</li>
</ul>
<p>The implementation takes advantage of Julia's notion of <em>multiple dispatch</em>
which nicely allows us to compare different policies.</p>
<p>Using this code we can train a Tic-Tac-Toe AI which you can play in the
browser:</p>
<div id="app"></div>
<script src="/tilly.js"></script>
<script>
    var node = document.getElementById("app");
    var app = Elm.Main.embed(node);
</script>
<p>This browser version is implemented <a href="http://elm-lang.org/">in Elm</a>. Check out the code <a href="https://github.com/egeromin/egeromin.github.io/blob/master/tilly.elm">on GitHub</a>. It uses a pre-trained
model that was trained using the Julia code.</p>
<h2>Limitations of Tabular Methods</h2>
<p>Monte Carlo algorithms and TD algorithms are both called <em>tabular</em> methods
because computing the value function requires computing a table of values for
all states, if we're computing <code>\(V\)</code>, or state-action pairs, if we're
computing <code>\(Q\)</code>. This is OK if we have a relatively limited number of states
or state-actions. In Tic-Tac-Toe, we have 2908 possible states, whereas in the
racetrack example, we have <code>\(9N\)</code> possible state-action pairs, where N is the
number of squares on the track. These numbers are still small and so tabular
methods apply. But what about games like Chess or Go, which have huge amounts
of states? In those cases, we cannot compute a value for every possible state
or state-action pair. Instead, we approximate the value function using function
approximation and machine learning. For example, the value function can be
modelled to be a neural network. Different methods apply in those cases and I
hope to cover them in a later blog post.</p>
<p>That's all, folks! For a much more comprehensive introduction to reinforcement
learning algorithms, check out Sutton and Barto's book &quot;Reinforcement
Learning&quot;, 2018 Edition, which introduces the theory gradually from the ground up.</p>
</section></article>

    <div id="footnotes">
        <hr />
        <ol>
            
            <li>(+1,+1), (+1, 0), (+1, -1), etc. </li>
            
            <li>Sutton and Barto, Chapters 3-5 </li>
            
            <li>adapted from Sutton and Barto, Section 5.4, page 101 </li>
            
            <li>this condition gives what's called  first-visit Monte Carlo . This is one variant of Monte Carlo prediction. Another is  multiple visits , which does not have this extra 'if' condition. </li>
            
            <li>Section 3.5, page 58 </li>
            
        </ol>
    </div>

    <div class="dates">
        First published 05 October 2018 at 09:21 UTC, <br />
        last updated 05 March 2019 at 09:58 UTC.
    </div>

</body>
</html>
